{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "185b4cbb",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Ye, L., Chi, M., Liu, Z.-W., Wang, X., & Gupta, V. (2024).\n",
    "  *Online mixed discrete and continuous optimization: Algorithms, regret analysis and applications*.\n",
    "  arXiv:2309.07630.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e5315c",
   "metadata": {},
   "source": [
    "# 1. General Problem Formulation\n",
    "\n",
    " $$\n",
    "\\begin{align}\n",
    "(S_t, x_t) \\xrightarrow{\\, f_t \\,} \\text{reward}_t := f_t(S_t,x_t)\n",
    " \\end{align}\n",
    " $$\n",
    "\n",
    "$$\n",
    "\\text{s.t.}\\quad\n",
    "\\left\\{\n",
    " \\begin{aligned}\n",
    " &(S_t, x_t) \\in \\mathcal{I} \\times \\mathcal{X}, \\qquad \\mathcal{X}\\subseteq \\mathbb{R}^d,\\\\\n",
    " &\\mathcal{I}=\\{S\\subseteq [n]: |S|\\le H\\}\\quad \\text{(uniform matroid)},\\\\\n",
    " &f_t:2^{[n]}\\times \\mathbb{R}^d \\to \\mathbb{R}_{\\ge 0},\\\\\n",
    " &f_t(\\cdot,\\cdot)\\ \\text{may depend on history }\\{(S_i,x_i)\\}_{i=1}^{t-1}.\n",
    "\\end{aligned}\n",
    " \\right.\n",
    "$$\n",
    "\n",
    "## 1.1 Environment \n",
    "\n",
    "- is allowed to choose multiple arms at each step\n",
    "- reward functions are unknown to the decision maker\n",
    "- reward functions are chosed based on the decision history\n",
    "- dynamic regret\n",
    "\n",
    "| Symbol | Definition |\n",
    "|-------|------------|\n",
    "| $\\mathcal{I}:=\\{S \\subseteq [n] : \\|S\\| \\le H \\}$ $H\\in \\mathbb{Z}_{\\ge 1}$| Feasible arm sets to choose |\n",
    "| $T \\in \\mathbb{Z}_{\\ge 1}$ | Step horizon |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc89c75e",
   "metadata": {},
   "source": [
    "\n",
    "## 1.2 Decision Variables\n",
    "\n",
    "| Symbol | Definition |\n",
    "|-------|------------|\n",
    "| $S_t \\in \\mathcal{I}$ | the set of arms chosed at step $t$|\n",
    "| $x_t \\in \\mathcal{X} \\subseteq \\mathbb{R}^d$ | the continuous variable chosed at step $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd96cc10",
   "metadata": {},
   "source": [
    "## 1.4 Alpha Regret (dynamic)\n",
    "\n",
    "$$\n",
    "R(\\alpha) :=\n",
    "\\mathbb{E}\\!\\left[\n",
    "\\alpha \\left(\n",
    "\\sum_{t=1}^{T} \\max_{S \\in \\mathcal{I},\\, x \\in \\mathcal{X}} f_t(S,x)\n",
    "\\right)\n",
    "- \\sum_{t=1}^{T} f_t(S_t, x_t)\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "s.t.\n",
    "- ðŸš¨ **dynamic regret**: \n",
    "\n",
    "$(S_t^\\star, x_t^\\star) \\in \\arg\\max_{S \\in \\mathcal{I},\\, x \\in \\mathcal{X}} f_t(S, x)$ may vary among $t\\in [T]$\n",
    "- the expectation is w.r.t the randomness of the online algorithm\n",
    "- $\\alpha \\in (0,1]$\n",
    "\n",
    " > **ðŸ“ŒDiscussion: Greedy-Algorithm-based optimal reward** \n",
    " >\n",
    " > A greedy algorithm is used to calculate the **approximately optimal reward**, i.e. $\\sum_{t=1}^{T} \\max_{S \\in \\mathcal{I},\\, x \\in \\mathcal{X}} f_t(S,x)$, however, this might not lead to the global optimal, depending on the specific function space where $f$ is chosen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a2fcd9",
   "metadata": {},
   "source": [
    "## 1.5 Model Assumptions\n",
    "\n",
    "**Assumption 1.**  \n",
    "The set $\\mathcal{X} \\subseteq \\mathbb{R}^d$ is convex, and there exist\n",
    "$r, D \\in \\mathbb{R}_{>0}$ such that\n",
    "$$\n",
    "0 \\in r \\mathbb{B} \\subseteq \\mathcal{X} \\subseteq D \\mathbb{B}.\n",
    "$$\n",
    "\n",
    "> ðŸ“Œ**Remark: A nicely bounded and full-dimensional continuous action space**\n",
    "\n",
    "**Assumption 2. Upper-Bouned and well-defined reward**  \n",
    "For any $S \\in \\mathcal{I}$ with $|S| = H$ and any $t \\in [T]$,\n",
    "$f_t(S,\\cdot)$ is concave and $G$-Lipschitz over $\\mathcal{X}$, and\n",
    "$$\n",
    "f_t(S, x) \\le C \\quad \\text{for all } x \\in \\mathcal{X},\n",
    "$$\n",
    "where $G, C \\in \\mathbb{R}_{\\ge 0}$.  \n",
    "\n",
    "> ðŸ“Œ**Remark**\n",
    ">\n",
    "> - concavity $\\Rightarrow$ a single global optimum in $x$ per $S$\n",
    "> - Lipschitzness $\\Rightarrow$ \n",
    "\n",
    "**Assumption 3.**  \n",
    "For any $t \\in [T]$, $f_t(\\cdot, x_t^\\star)$ is monotone nondecreasing with\n",
    "$$\n",
    "f_t(\\varnothing, x_t^\\star) = 0.\n",
    "$$\n",
    "\n",
    "> ðŸ“Œ**Discussion: A strong assumption**\n",
    ">\n",
    "> This is a relatively strong assumption in applications, one should check whether the situation align with this assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736166ff",
   "metadata": {},
   "source": [
    "## 1.6 Information Patterns\n",
    "\n",
    "### Case 1. single-point feedback\n",
    "\n",
    "for $\\forall t \\in [T]$:\n",
    "$$\n",
    "\\text{choose} ~(S_t,x_t) \\longrightarrow \\text{observe} ~ f_t(S_t,x_t)  \n",
    "$$\n",
    "\n",
    "### Case 2. multi-point feedback\n",
    "\n",
    "for $\\forall t \\in [T]$:\n",
    "$$\n",
    "\\text{choose} ~(S_t,x_t) \\longrightarrow \\text{observe} ~ \\{f_t(S,x) | (S,x) \\in \\{\\text{A set containing}(S_t,x_t)\\}\\}  \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd72d85f",
   "metadata": {},
   "source": [
    "# 2. MAB problem with error feed back"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f287d32",
   "metadata": {},
   "source": [
    "## 2.1 Problem Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c2e780",
   "metadata": {},
   "source": [
    "### 2.1.1 Environment\n",
    "\n",
    "- adversarial reward\n",
    "- erroneous observation\n",
    "\n",
    "| Symbol | Definition |\n",
    "|-------|------------|\n",
    "| $\\mathcal{I}:= [n]$ | Feasible arms to choose |\n",
    "| $T \\in \\mathbb{Z}_{\\ge 1}$ | Step horizon |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecce2157",
   "metadata": {},
   "source": [
    "### 2.1.2 Decision Variables\n",
    "\n",
    "| Symbol | Definition |\n",
    "|-------|------------|\n",
    "| $i_t$| arm to choose at time step $t$ |\n",
    "\n",
    "- $i := (i_1, i_2, \\cdots, i_T) \\in \\mathcal{I}^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20d9b4c",
   "metadata": {},
   "source": [
    "### 2.1.3 Adversarial Reward\n",
    "\n",
    "Reward vector at step $t \\in [T]$: \n",
    "$$\n",
    "r_t := (r_t^1, \\ldots, r_t^n) \\in \\mathbb{R}^n_{\\ge 0}\n",
    "$$ \n",
    "\n",
    "\n",
    "where $r_t = \\phi_t(i_1, i_2, \\ldots, i_{t-1})$ is an **Adversarial reward mapping**, $\\phi_t : \\mathcal{I}^{t-1} \\rightarrow \\mathbb{R}^n_{\\ge 0}$\n",
    "\n",
    "> **ðŸ“ŒRemark: Definition of the adversarial reward mapping**\n",
    ">\n",
    "> A reward mapping is adversarial if it is not assumed to be stochastic or stationary and may depend on the entire past action history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a3ac48",
   "metadata": {},
   "source": [
    "### 2.1.4 Dynamic Regret\n",
    "\n",
    "$$R_{\\mathcal{M}}(i^*) := \\mathbb{E}\\left[\\sum_{t=1}^T r^{i^*_t}_t - \\sum_{t=1}^T r_t^{i_t}\\right]$$\n",
    "\n",
    "where:\n",
    "- The expectation is taken with respect to the randomness in the online algorithm used by the decision maker\n",
    "\n",
    "- $i^* := (i_1^*, i_2^*, \\cdots, i_T^*)\\in \\mathcal{I}^T$ \n",
    "given by the clairvoyant\n",
    "\n",
    "> **ðŸ“ŒDiscussion â€” Clairvoyant Benchmark**\n",
    ">\n",
    "> How does the clairvoyant know $i_t^*$ in practice while given the reward function?\n",
    ">\n",
    "> One possible way is a greedy rule:\n",
    "> $$\n",
    "> i_t^* := \\arg\\max_{i\\in\\mathcal{I}} r_t^i(i_1^\\star,\\dots,i_{t-1}^*),\n",
    "> $$\n",
    "> which may fail to achieve global optimality.\n",
    ">\n",
    "> Another way is global optimization:\n",
    "> $$\n",
    "> i^* := \\arg\\max_{i\\in\\mathcal{I}^T}\\sum_{t=1}^T r_t^{i_t},\n",
    "> $$\n",
    "> which is typically intractable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2707b7",
   "metadata": {},
   "source": [
    "### 2.1.5 Variability of $i^*$\n",
    "\n",
    "A metric to measure the variability of $i^*_t$, $i^*$ with high $V_T^{i^*}$ may be harder to handle.\n",
    "\n",
    "$$\n",
    "V_T^{i^*}\n",
    "\\;=\\;\n",
    "1 \\;+\\; \\sum_{t=1}^{T-1}\n",
    "\\mathbb{I}\\!\\left\\{\\, i_t^* \\neq i_{t+1}^\\star \\,\\right\\} \n",
    "$$\n",
    "\n",
    "- $range(V_T^{\\cdot})= \\{1,2,\\cdots,T\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea47231",
   "metadata": {},
   "source": [
    "### 2.1.6 Erroneous Observation\n",
    "\n",
    "For each round $t$, the decision maker observe an reward with uncertainty and noises:\n",
    "\n",
    "$$\n",
    "\\tilde{r}_t^{i_t} := \\beta_t(r_t^{i_t} - \\epsilon_t^{i_t} + \\epsilon_t)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\epsilon_t \\in \\mathbb{R}$ is the noise w.r.t round $t$\n",
    "- $\\epsilon_t^{i_t} \\in \\mathbb{R}$ is the noise w.r.t arm $i_t$\n",
    "- $\\beta_t \\sim Bernoulli(\\rho)$\n",
    "- $\\epsilon_t, \\epsilon_t^{i_t}, \\beta_t$ are i.i.d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ae74c2",
   "metadata": {},
   "source": [
    "## 2.2 Algorithm: EXP3.S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4ce817",
   "metadata": {},
   "source": [
    "![Algorithm1 line3](./materials/Algorithm1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75426b7a",
   "metadata": {},
   "source": [
    "### 2.2.1 Methematical Analysis to the Algorithm\n",
    "\n",
    "> ðŸ“Œ**line3 in Algorithm1**\n",
    ">\n",
    "> One may deem $\\gamma$ as the **\"confidence\"** to the null hypothesis: \"Each arm is rewarded identically\",under which, the best policy is a uniform-distribution,and this explains the $\\frac{\\gamma}{n}$ part of the formula.\n",
    ">\n",
    ">![Algorithm1 line3](./materials/Algorithm1_line3_analysis.png)\n",
    ">\n",
    ">BTW, for $\\gamma \\in (0,1)$, we have:\n",
    ">$$\n",
    ">\\min\\{\\frac{\\tilde{w}_t^i}{\\sum_{j=1}^n \\tilde{w}_t^j}, \\frac{1}{n}\\} < p_t^i < \\max\\{\\frac{\\tilde{w}_t^i}{\\sum_{j=1}^n \\tilde{w}_t^j}, \\frac{1}>{n}\\} \n",
    ">$$\n",
    "\n",
    "> ðŸ“Œ**line7 in Algorithm1** \n",
    ">\n",
    ">observe that $\\hat{r_t}^j$ is somehow unbiased, the proof is as follows: \n",
    ">$$\n",
    ">\\begin{align}\n",
    ">{\\mathbb{E}}(\\hat{r_t}^j) &= \\mathbb{E}(\\beta_t)\\frac{\\!\\left(r_t^j - >\\epsilon_t^j + \\epsilon_t - a\\right)}{p_t^j (b-a)}p_t^j  \\\\\n",
    ">&= \\frac{\\rho\\!\\left(r_t^j - \\epsilon_t^j + \\epsilon_t - a\\right)}{(b-a)}\\\\\n",
    ">\\end{align}\n",
    ">$$\n",
    ">\n",
    ">here the designer made a min-max standardization to the reward with noises\n",
    "\n",
    "> **ðŸ“Œline 7 & 8 in Algorithm1**\n",
    ">\n",
    "> Set \n",
    ">$$f(r^{j}_t, p^{j}_t, \\beta_t):= \\frac{\\beta_t(r_t^j - \\epsilon_t^j +\\epsilon_t -a)}{p_t^j(b-a)}$$\n",
    "> $$g_t(\\hat{r_t}^j):= \\omega_t^{\\,j}\\exp\\!\\left(\\frac{\\gamma \\,\\hat r_t^{\\,j}}{n}\\right)+\\frac{e}{nT}\\sum_{j=1}^{n}\\omega_t^{\\,j}$$\n",
    "> \n",
    "> **ðŸŸ¥ We suppose that the erroneous observation $\\beta_t(r_t^j - \\epsilon_t^j + \\epsilon_t -a)$ always be non-negative for the convinience of the following analysis.**\n",
    "> \n",
    ">\n",
    "> **(1) Firstly let's take a look at these functions themselves:**\n",
    ">\n",
    "> **(i) for $f(r^{j}_t, p^{j}_t, \\beta_t)$:**\n",
    "> \n",
    "> break this function into two multiplying parts: $\\frac{\\beta_t}{p_t^j} \\cdot \\frac{(r_t^j - \\epsilon_t^j +\\epsilon_t -a)}{(b-a)}$\n",
    "> - $\\frac{(r_t^j - \\epsilon_t^j +\\epsilon_t -a)}{(b-a)}$: a min-max standardization to the reward with noises\n",
    "> - $\\frac{\\beta_t}{p_t^j}$: when $\\beta_t = 1$, $p_t^j \\rightarrow 0$, $\\frac{\\beta_t}{p_t^j} \\rightarrow + \\infty$, this property will be further discussed in following part (2).\n",
    "> \n",
    "> **(ii) for $g_t(\\hat{r_t}^j)$:**\n",
    "> - $\\omega_t^{\\,j}\\exp\\!\\left(\\frac{\\gamma \\,\\hat r_t^{\\,j}}{n}\\right)$: since $\\frac{\\gamma \\,\\hat r_t^{\\,j}}{n} \\ge 0$ $\\Rightarrow$ $\\omega_t^{\\,j}\\exp\\!\\left(\\frac{\\gamma \\,\\hat r_t^{\\,j}}{n}\\right) \\ge \\omega_t^{\\,j}$, this ðŸŸ¥non-decreasing property will be discussed in the following part (2).  Apart from that, since the derivative $\\frac{d\\,\\exp\\!\\left(\\frac{\\gamma \\,\\hat r_t^{\\,j}}{n}\\right)}{d\\hat{r_t}^{\\,j}}$ scales exponentially with $\\hat{r_t}^{\\,j}$, and hence exhibits super-linear (indeed exponential) growth, implying ðŸŸ¥**high sensitivity to large estimated rewards**. \n",
    "> \n",
    "> - $\\frac{1}{T}\\frac{\\sum_{j=1}^{n}(e\\cdot \\omega_t^{\\,j})}{n}$: the lower bound of $g$ to garuantee that no arm dies along steps.\n",
    ">\n",
    "> ****\n",
    ">\n",
    "> **(2) Machanism analysis to $f$ and $g$ :**\n",
    ">\n",
    "> Suppose arm $j$ is chosen at step $t$, then \n",
    "> $$\\hat{r}_t^j = f(r^{j}_t, p^{j}_t, \\beta_t)$$\n",
    ">Then we try to discuss the behaviour and interaction between the two fuctions w.r.t  $r^{j}_t, p^{j}_t,\\beta_t$ and study the rationale behind this design:\n",
    ">\n",
    "> **(i) If $\\beta_t = 1$**:\n",
    "> - **when $r_t^j$  is relatively big while $p_t^j$ is small**: This means arm $j$ is previouly under-estimated (high reward, low choosing probability) $\\Rightarrow$ $\\hat{r}_t^j =f(r^{j}_t, p^{j}_t, \\beta_t)$ is big $\\Rightarrow$ $g(\\hat{r}_t^j)$ is big due to the sensativity of $exp$ to large value of $\\hat{r}_t^j$ $\\Rightarrow$ $\\omega_{t+1}^j = g$ is big $\\Rightarrow$ $p_{t+1}^i$ will raise significantly $\\Rightarrow$ the under-estimation issue get reduced.\n",
    "> \n",
    "> - **when $r_t^j$ is small and $p_t^j$ is large**: This means arm $j$ is heavily over-estimated,however, ðŸŸ¥**this will not lead to a reduction of $p_{t+1}^i$ as significant as the case obove.** To prove this, suppose $\\hat{r}_t^j \\rightarrow 0$, then $\\omega_{t+1}^j = g(\\hat{r}_t^j)\\rightarrow \\omega_{t}^j + \\frac{1}{T}\\frac{\\sum_{j=1}^{n}(e\\cdot \\omega_t^{\\,j})}{n} \\ge \\omega_{t}^j$, this lead to a merely static effect to $p_{t}^j$ as well as other $p_{t}^i$'s. In short, ðŸŸ¥**the algorithm chooses to make a minimal upgrade to $p_{t}^j$ rather than decrease $p_{t}^j$ imediately, thought arm $j$ is found to be over-estimated**, however, ðŸŸ¥this over-estimated arm will be gradually less-chosen due to its steady upgrade while other arms are upgrading.\n",
    ">\n",
    "> **(ii) If $\\beta_t = 0$:**\n",
    "> \n",
    "> - similar to the case above where arm $j$ is over-estimated, the algorithm won't reduce $p_t^j$, ðŸŸ¥**this strategy avoid making aggresive updates when the reward is mis-observed to 0.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54702cc7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "552c2868",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
